{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2ajAcoeRyvV2+e++9CSyB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdCeiY-E-1Ei"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activation='relu', learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Initialize the network.\n",
        "        :param layer_sizes: List of layer sizes, e.g., [input_size, hidden1_size, ..., output_size]\n",
        "        :param activation: 'relu' or 'sigmoid'\n",
        "        :param learning_rate: Learning rate for gradient descent\n",
        "        \"\"\"\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation = activation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize weights and biases (He initialization for ReLU, Xavier for sigmoid)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            if activation == 'relu':\n",
        "                scale = np.sqrt(2.0 / layer_sizes[i])\n",
        "            else:  # sigmoid\n",
        "                scale = np.sqrt(1.0 / layer_sizes[i])\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * scale)\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i+1])))  # Row vector\n",
        "\n",
        "    def _activate(self, x):\n",
        "        \"\"\"Apply activation function.\"\"\"\n",
        "        if self.activation == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        return x  # Linear (output layer)\n",
        "\n",
        "    def _activate_derivative(self, x):\n",
        "        \"\"\"Derivative of activation function.\"\"\"\n",
        "        if self.activation == 'relu':\n",
        "            return (x > 0).astype(float)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            s = self._activate(x)\n",
        "            return s * (1 - s)\n",
        "        return np.ones_like(x)  # Linear (output layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass through all layers.\"\"\"\n",
        "        self.layer_outputs = [X]  # Store outputs of each layer (including input)\n",
        "        for w, b in zip(self.weights, self.biases):\n",
        "            X = np.dot(X, w) + b\n",
        "            X = self._activate(X)\n",
        "            self.layer_outputs.append(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, X, y_true):\n",
        "        \"\"\"Backpropagation to compute gradients.\"\"\"\n",
        "        m = X.shape[0]  # Number of samples\n",
        "        y_pred = self.layer_outputs[-1]\n",
        "\n",
        "        # Initialize gradients\n",
        "        dW = [np.zeros_like(w) for w in self.weights]\n",
        "        db = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        # Output layer error (assuming MSE loss)\n",
        "        error = (y_pred - y_true) / m  # dL/dZ for output layer\n",
        "        if self.activation == 'sigmoid':\n",
        "            error *= self._activate_derivative(self.layer_outputs[-1])  # Chain rule\n",
        "\n",
        "        # Backpropagate through layers\n",
        "        for l in range(len(self.weights)-1, -1, -1):\n",
        "            # Gradient of weights/biases\n",
        "            dW[l] = np.dot(self.layer_outputs[l].T, error)\n",
        "            db[l] = np.sum(error, axis=0, keepdims=True)\n",
        "\n",
        "            # Propagate error backward (skip for input layer)\n",
        "            if l > 0:\n",
        "                error = np.dot(error, self.weights[l].T) * self._activate_derivative(self.layer_outputs[l])\n",
        "\n",
        "        return dW, db\n",
        "\n",
        "    def update_weights(self, dW, db):\n",
        "        \"\"\"Update weights using gradients.\"\"\"\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * dW[i]\n",
        "            self.biases[i] -= self.learning_rate * db[i]\n",
        "\n",
        "    def train(self, X, y_true, epochs=1000):\n",
        "        \"\"\"Train the network.\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            dW, db = self.backward(X, y_true)\n",
        "            self.update_weights(dW, db)\n",
        "\n",
        "            # Print loss (MSE) every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                loss = np.mean((y_pred - y_true)**2)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "2SGo3HP6_Za4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: XOR problem (2 inputs, 1 output)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create network (2 inputs, 2 hidden layers of 4 neurons, 1 output)\n",
        "nn = NeuralNetwork(layer_sizes=[2, 4, 4, 1], activation='relu', learning_rate=0.01)\n",
        "\n",
        "# Train\n",
        "nn.train(X, y, epochs=1000)\n",
        "\n",
        "# Test\n",
        "print(\"Predictions:\", nn.forward(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lNuxzsCNuTi",
        "outputId": "b6fc3565-eacd-4117-b315-fb51c6f1a79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6128\n",
            "Epoch 100, Loss: 0.2299\n",
            "Epoch 200, Loss: 0.1791\n",
            "Epoch 300, Loss: 0.1443\n",
            "Epoch 400, Loss: 0.1074\n",
            "Epoch 500, Loss: 0.0760\n",
            "Epoch 600, Loss: 0.0508\n",
            "Epoch 700, Loss: 0.0322\n",
            "Epoch 800, Loss: 0.0200\n",
            "Epoch 900, Loss: 0.0151\n",
            "Predictions: [[0.18066033]\n",
            " [0.89307042]\n",
            " [0.98364135]\n",
            " [0.02154583]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WB9CPqb0NxKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}